\documentclass{article}
\usepackage{LaTeX-Submodule/template}
\usepackage{changepage} % Modify page width
\usepackage{multicol}

\usepackage{titlesec}

\geometry{
	a4paper,
	margin = 10mm
} 

\usepackage{calc}

\usepackage{textcomp, upquote}

\lstset{language=Matlab, upquote=true}
\lstset{morekeywords={randi, false, imshow, drawpolygon, polyarea, drawline, ones, imread, VideoWriter, XData, YData, getFrame, writeVideo, deg2rad, soundsc, resample, audiowrite, sind}}
\lstset{
    basicstyle = \ttfamily, columns = fullflexible, keepspaces = true
}

\pagenumbering{gobble}

\setlength\columnsep{4pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt}

\titleformat*\subsection{\raggedright\bfseries\large}
\titleformat*\subsubsection{\raggedright\bfseries}

\begin{document}
\titlespacing*\subsubsection{0pt}{1ex}{1ex}
\titlespacing*\subsection{0pt}{0ex}{0ex}

\setlength{\abovecaptionskip}{-5pt}
\setlength{\textfloatsep}{0pt}

\setlength{\abovedisplayskip}{1pt}
\setlength{\belowdisplayskip}{1pt}

\begin{multicols}{3}
    \subsection*{MATLAB}
    \lstset{belowskip=0pt, aboveskip=0pt}
    \begin{lstlisting}
% Function declaration
function [output1, ...]
    = function_name(input1, ...)
% Iterate over the vector v
for i = v ... end 
% Repeat while condition is true
while condition ... end
% Execute first true alternative
if condition_1 ...
elseif condition_2 ... 
else ... end
\end{lstlisting}
    \subsubsection*{Partial Fraction Decomposition}
    Given the LHS in the denominator, substitute the RHS.
    \begin{align*}
        \left(ax+b\right)^k & \to \frac{A_1}{ax+b} + \cdots + \frac{A_k}{\left( ax+b \right)^k}
    \end{align*}
    \begin{align*}
        \left(ax^2+bx+c\right)^k     & \to                                                  \\
        \frac{A_1x+B_1}{ax^2+bx+c} + & \cdots + \frac{A_kx+B_k}{\left( ax^2+bx+c \right)^k}
    \end{align*}
    \subsection*{Differential Equations}
    \subsubsection*{Mechanical Systems}
    \begin{equation*}
        \sum F = \dv{p}{t},\quad \sum M = I \dv[2]{\theta}{t}
    \end{equation*}
    where momentum $p = mv$, moment $M=Fx$ and inertia $I=mr^2$.
    % \columnbreak
    \begin{align*}
        F_T & = \left( c - v \right) d_f & F_g & = -mg                       \\
        F_s & = -kx                      & F_f & = -b v^2 \text{ (or $-bv$)}
    \end{align*}
    \subsubsection*{Electrical Circuits}
    \begin{align*}
        i = \dv{q}{t},\quad \sum v_{loop} = 0,\quad \sum i_{node} = 0
    \end{align*}
    \underline{Voltage drop across elements:}
    \begin{align*}
        v_R = iR,\quad v_C = \frac{q}{C},\quad v_L & = L \dv{i}{t}
    \end{align*}
    \subsubsection*{Separable ODEs}
    For $\dv{y}{t} = p(t) q(y)$:
    \begin{equation*}
        \int \frac{1}{q(y)} \dv{y}{t} \dd{t} = \int p(t) \dd{t}.
    \end{equation*}
    \subsubsection*{Linear ODEs}
    For $\dv{y}{t} + p(t)y = q(t)$, use the \textit{integrating factor}:
    $I(t) = \e^{\int p(t) \dd{t}}$, so that
    \begin{equation*}
        y(t) = \frac{1}{I(t)} \int I(t) q(t) \dd{t}.
    \end{equation*}
    \subsubsection*{Linearisation}
    \begin{align*}
        f(t)              & \approx f(t_0) + f'(t_0)(t-t_0)                                 \\
        f\bigl(y(t)\bigr) & \approx f\bigl(y(t_0)\bigr)                                     \\
                          & \phantom{\approx} + f'\bigl(y(t_0)\bigr)\bigl(y(t)-y(t_0)\bigr)
    \end{align*}
    \subsubsection*{Euler's Method}
    For $\symbfit{y}' = \symbfit{f}(t,\: \symbfit{y})$:
    \begin{align*}
        \symbfit{y}_{n+1} & = \symbfit{y}_{n} + h\symbfit{f}(t_n,\: \symbfit{y}_n)
    \end{align*}
    \subsubsection*{Modified Euler's Method}
    \begin{align*}
         & \symbfit{y}_{n+1} =                                                                                                       \\
         & \symbfit{y}_{n} + \frac{h}{2}\left( \symbfit{f}(t_n,\: \symbfit{y}_n) + \symbfit{f}(t_{n+1},\: \symbfit{y}_{n+1}) \right)
    \end{align*}
    where $\symbfit{f}(t_{n+1},\: \symbfit{y}_{n+1})$ is determined using Euler's method.
    \subsubsection*{Second-Order ODEs}
    \begin{equation*}
        ay'' + by' + cy = F(t)
    \end{equation*}
    \subsubsection*{General Solution}
    \begin{equation*}
        y(t) = y_H(t) + y_P(t)
    \end{equation*}
    \subsubsection*{Homogeneous Solution}
    \begin{equation*}
        y_H(t) = \e^{\lambda t}
    \end{equation*}
    To solve for $\lambda$, substitute the homogeneous form into the homogeneous ODE. \\
    \underline{Real distinct roots:}
    \begin{equation*}
        y_H(t) = c_1\e^{\lambda_1 t} + c_2\e^{\lambda_2 t}
    \end{equation*}
    \underline{Real repeated roots:}
    \begin{equation*}
        y_H(t) = c_1\e^{\lambda t} + c_2 t\e^{\lambda t}
    \end{equation*}
    \underline{Complex conjugate roots:} $\lambda = \alpha \pm \beta i$
    \begin{equation*}
        y_H(t) = \e^{\alpha x}\bigl( c_1\cos{\left( \beta t \right)} + c_2 \sin{\left( \beta t \right)} \bigr)
    \end{equation*}
    \subsubsection*{Particular Solution}
    \emph{See table below.}
    Substitute $y_P$ into the nonhomogeneous ODE, and solve the undetermined coefficients.
    \subsubsection*{System of ODEs}
    Given $\symbfit{y}' = \symbf{A} \symbfit{y}$,
    \begin{equation*}
        \symbfit{y}_H(t) = \symbfit{q}\e^{\lambda t}.
    \end{equation*}
    $\lambda_i$ are the eigenvalues of $\symbfit{A}$ that satisfy
    \begin{equation*}
        \det{\left( \symbf{A} - \lambda\symbf{I} \right)} = 0.
    \end{equation*}
    $\symbfit{q}_i$ are the associated eigenvectors that satisfy
    \begin{equation*}
        \left( \symbf{A} - \lambda_i \symbf{I} \right)\symbfit{q}_i = \symbf{0}.
    \end{equation*}
    \underline{For real distinct roots:}
    \begin{equation*}
        \symbfit{y}_H(t) = c_1\symbfit{q}_1\e^{\lambda_1 t} + c_2\symbfit{q}_2\e^{\lambda_2 t}
    \end{equation*}
    \subsubsection*{Higher-Order ODEs}
    \begin{equation*}
        y^{\left( n \right)} + a_1 y^{\left( n-1 \right)} + \cdots + a_{n-1} y' + a_n y = 0
    \end{equation*}
    Let $y_1 = y$, $y_2 = y'$, \dots, $y_n = y^{\left( n-1 \right)}$
    so that $\symbfit{y}=\avec{y_1,\: y_2,\: \dots,\: y_n}$.
    Then $\symbfit{y}' = \symbf{A} \symbfit{y}$, where
    \begin{equation*}
        \symbf{A} =
        \mqty[
        0      & 1        & 0        & \cdots & 0      \\
        0      & 0        & 1        & \cdots & 0      \\
        \vdots & \vdots   & \vdots   & \ddots & \vdots \\
        0      & 0        & 0        & \cdots & 1      \\
        -a_n   & -a_{n-1} & -a_{n-2} & \cdots & -a_1
        ]
    \end{equation*}
\end{multicols}
\begin{table}[H]
    \centering
    \begin{tabular}{c | c}
        \toprule
        $F(t)$                                                             & $y_P(t)$                                                                \\
        \midrule
        a constant                                                         & A                                                                       \\
        a polynomial of degree $n$                                         & $\displaystyle \sum_{i = 0}^n A_i t^i$                                  \\
        $\e^{kt}$                                                          & $A \e^{kt}$                                                             \\
        $\cos{\left( \omega t \right)}$ or $\sin{\left( \omega t \right)}$ & $A_0 \cos{\left( \omega t \right)} + A_1 \sin{\left( \omega t \right)}$ \\
        a combination of the above                                         & a combination of the above                                              \\
        linearly dependent to $y_H(t)$                                     & multiply $y_P(t)$ by $t$ until linearly independent                     \\
        \bottomrule
    \end{tabular}
\end{table}
\begin{multicols}{3}
    \subsection*{Probability}
    \subsubsection*{Events}
    \begin{gather*}
        \Pr{\left( A \right)} = \sum_{x\in A} p(x) \\
        \Pr{\left( A^C \right)} = \Pr{\left( \overline{A} \right)} = 1 - \Pr{\left( A \right)}
    \end{gather*}
    \subsubsection*{Unions}
    \begin{align*}
        \Pr{\left( A \cup B \right)} & = \Pr{\left( A \right)} + \Pr{\left( B \right)} \\
                                     & \phantom{=} - \Pr{\left( A \cap B \right)}      \\
                                     & = 1 - \Pr{\left( A^C \cap B^C \right)}
    \end{align*}
    \subsubsection*{Bayes' Theorem}
    \begin{equation*}
        \Pr{\left(B \;\middle|\; A\right)} = \frac{\Pr{\left( A \;\middle|\; B \right)}\Pr{\left( B \right)}}{\Pr{\left( A \right)}}
    \end{equation*}
    \subsubsection*{Disjoint Events}
    Events don't have outcomes in common.
    \begin{align*}
        A \cap B                   & = \varnothing \\
        \Pr{\left(A \cap B\right)} & = 0
    \end{align*}
    \subsubsection*{Independent Events}
    Outcome of events do not influence each other. Joint probability:
    \begin{equation*}
        \Pr{\left(A \cap B\right)} = \Pr{\left( A \right)}\Pr{\left( B \right)}
    \end{equation*}
    \subsubsection*{Dependent (Conditional) Events}
    Outcome of event depends on the outcome of the other. Joint probability of $A$ given $B$:
    \begin{equation*}
        \Pr{\left(A \;\middle|\; B\right)} = \frac{\Pr{\left( A \cap B \right)}}{\Pr{\left( B \right)}}
    \end{equation*}
    \subsubsection*{Total Probability}
    For disjoint events $B_i$:
    \begin{align*}
        A                     & = \bigcup\limits_{i=1}^n \left( A \cap B_i \right) \\
        \Pr{\left( A \right)} & = \sum_{i=1}^n \Pr{\left( A \cap B_i \right)}
    \end{align*}
    \subsubsection*{Expectation}
    Average output or mean:
    \begin{equation*}
        \mu = \E{\left( X \right)}.
    \end{equation*}
    \subsubsection*{Variance}
    Measure of spread from the mean:
    \begin{equation*}
        \sigma^2 = \E{\left( X^2 \right)} - \E{\left( X \right)}^2.
    \end{equation*}
\end{multicols}
\begin{minipage}{126.1962963mm}
    \begin{multicols}{2}
        \subsection*{Probability Distributions}
        \subsubsection*{Discrete Random Variables}
        Has countably many outcomes. Distributed with a
        probability mass function $p(x)$.
        \subsubsection*{Continuous Random Variables}
        Has an infinite number of outcomes. Distributed with a
        probability density function $f(x)$.
        \begin{equation*}
            \Pr{\left( x_1 \leq X \leq x_2 \right)} = \int_{x_1}^{x_2} f(x) \dd{x}
        \end{equation*}
        \begin{equation*}
            F(x) = \Pr{\left( X \leq x \right)}
        \end{equation*}
        \subsubsection*{General Linear Combinations}
        \begin{align*}
            \E{\left( aX\pm b \right)}  & = a\E{\left( X \right)} \pm b                     \\
            \E{\left( aX\pm bY \right)} & = a\E{\left( X \right)} \pm b\E{\left( Y \right)}
        \end{align*}
        \begin{align*}
            \Var{\left( aX\pm b \right)} & = a^2\Var{\left( X \right)}                  \\
            \Var{\left( aX\pm bY \right)}   & = a^2\Var{\left( X \right)}                  \\
                                         & \phantom{=} + b^2\Var{\left( Y \right)}      \\
                                         & \phantom{=} \pm 2ab\Cov{\left( X,\: Y \right)}
        \end{align*}
        \begin{align*}
            \Cov{\left( X,\: Y \right)} & = \E{\left( XY \right)} - \E{\left( X \right)}\E{\left( Y \right)} \\
                                        & = \rho_{XY} \sqrt{\Var{\left( X \right)}\Var{\left( Y \right)}}.
        \end{align*}
        The correlation $\Corr{\left( X,\: Y \right)}$ or $\rho_{XY}$ is a constant that describes the statistical relationship
        between $X$ and $Y$. $-1 \leq \rho_{XY} \leq 1$.
    \end{multicols}
    \begin{table}[H]
        \centering
        \begin{tabular}{c | c | c}
            \toprule
                                   & \textbf{Discrete}                            & \textbf{Continuous}                                \\
            \midrule
            Valid probabilities    & $0 \leq p(x) \leq 1$                         & $f(x) \geq 0$                                      \\
            Cumulative probability & $\sum_{u \leq x} p(u)$                       & $\int_{-\infty}^{x} f(u) \dd{u}$                   \\
            Expectation            & $\sum_{\Omega} xp(x)$                        & $\int_{\Omega} xf(x)\dd{x}$                        \\
            Variance               & $\sum_{\Omega} \left( x - \mu \right)^2p(x)$ & $\int_{\Omega} \left( x - \mu \right)^2f(x)\dd{x}$ \\
            \bottomrule
        \end{tabular}
    \end{table}
\end{minipage}\hfill%
\begin{minipage}{62.39259259mm}
    \subsubsection*{Binomial (Discrete)}
    Probability of $x$ successes out of $n$ independent trials, each with chance $p$.
    \subsubsection*{Poisson (Discrete)}
    Probability of observing $x$ events over an interval $t$, where events occur at an
    average rate $\lambda$. $\mu = \lambda t$.
    \subsubsection*{Exponential (Continuous)}
    The time $t$ between events, where the events are independent and occur at an average rate $\lambda$.
    \subsubsection*{Uniform (Continuous)}
    The probability of any value $x\in\left[ a,\: b \right]$ is constant.
    \subsubsection*{Normal (Continuous)}
    Events occur more frequently near $\mu$ and less frequently further away from $\mu$.
    \subsubsection*{Standardised Normal (Continuous)}
    \begin{equation*}
        Z = \frac{X-\mu}{\sigma} \sim \mathrm{N}\left( 0,\: 1 \right)
    \end{equation*}
\end{minipage}
\begin{table}[H]
    \centering
    \begin{tabular}{c | c | c | c | c}
        \toprule
        \textbf{Distribution}                                    & \textbf{Probability}                            & \textbf{Cumulative Probability}         & $\mu$                  & $\sigma^2$                \\
        \midrule
        Binomial: $X\sim \mathrm{bin}\left( n,\: p \right)$      & $\binom{n}{x} p^x \left( 1 - p \right)^{n - x}$ & \emph{See table above.}                 & $np$                   & $np\left( 1-p \right)$    \\
        Poisson: $X\sim \mathrm{Pois}\left( \mu \right)$         & $\e^{-\mu}\mu^x/x!$                             & \emph{See table above.}                 & $\lambda t$            & $\lambda t$               \\
        Exponential: $T\sim \mathrm{exp}\left( \lambda \right)$  & $\lambda \e^{-\lambda t}$                       & $1 - \lambda \e^{-\lambda t}$           & $1/\lambda$            & $1/\lambda^2$             \\
        Uniform: $X\sim \mathrm{U}\left( a,\: b \right)$         & $1/\left( b-a \right)$                          & $\left( x-a \right)/\left( b-a \right)$ & $\left( a+b \right)/2$ & $\left( b-a \right)^2/12$ \\
        Normal: $X\sim \mathrm{N}\left( \mu,\: \sigma^2 \right)$ & $-$                                             & $-$                                     & $\mu$                  & $\sigma^2$                \\
        \bottomrule
    \end{tabular}
\end{table}
\begin{minipage}{62.39259259mm}
    \subsection*{Sample Statistics}
    Given $n$ samples $x_i$:
    \begin{align*}
        \text{Mean:}     &  & \overline{x} & = \frac{1}{n}\sum_{i=1}^{n} x_i                                      \\
        \text{Variance:} &  & s^2          & = \frac{1}{n-1}\sum_{i=1}^{n} \left( x_i - \overline{x} \right)^2    \\
                         &  &              & = \frac{1}{n-1}\left( \sum_{i=1}^{n} x_i^2 - n\overline{x}^2 \right)
    \end{align*}
    \subsubsection*{t Distribution}
    The sample mean is distributed according to the t distribution where
    \begin{equation*}
        T=\frac{\overline{X} - \mu}{s/\sqrt{n}}\sim t_{n-1}
    \end{equation*}
    \begin{align*}
        \Pr{\left( T \leq t_{n-1,\: 1-\alpha/2} \right)} & = 1 - \frac{\alpha}{2} \\
        \Pr{\left( T > t_{n-1,\: \alpha/2} \right)}      & = \frac{\alpha}{2}
    \end{align*}
\end{minipage}\hfill%
\begin{minipage}{126.1962963mm}
    \begin{multicols*}{2}
        \subsubsection*{Confidence Intervals}
        Given the confidence level $c$:
        \begin{equation*}
            c = 1 - \alpha.
        \end{equation*}
        The confidence interval for $\overline{x}$:
        \begin{equation*}
            {CI}_{c} = \overline{x} \pm t_{n-1,\: 1-\alpha/2} \frac{s}{\sqrt{n}}.
        \end{equation*}
        \subsection*{Hypothesis Testing}
        Hypothesis testing assesses the likelihood of observing the sample \underline{if} the null hypothesis was true.
        \subsubsection*{Hypothesis}
        \begin{align*}
            H_0:\mu = \mu_0 \quad \text{vs} \quad H_1:\mu \neq \mu_0
        \end{align*}
        Generally there is no evidence supporting $H_0$, but only evidence or lack of evidence for rejecting $H_0$.
        \subsubsection*{Test Statistic}
        The measure of the distance that the proposed mean is from the sample mean:
        \begin{align*}
            T_{\text{test}} = \frac{\overline{x} - \mu_0}{s/\sqrt{n}}
        \end{align*}
        \subsubsection*{p-Value}
        Strength of evidence against $H_0$. The $p$-value is the $\alpha$ value that satisfies:
        \begin{equation*}
            \abs{T_{\text{test}}} = t_{n-1,\: 1-\alpha/2}.
        \end{equation*}
        \begin{align*}
            \Pr{\left( T \leq \abs{T_{\text{test}}} \right)} & = 1 - \frac{\alpha}{2} \\
            \Pr{\left( T > \abs{T_{\text{test}}} \right)}    & = \frac{\alpha}{2}
        \end{align*}
    \end{multicols*}
    \includegraphics[width = \linewidth, keepaspectratio = true]{p_values}
\end{minipage}
\begin{multicols}{3}
    \subsection*{Linear Regression}
    Given a set of $n$ points $\left( x_i,\: y_i \right)$ that are assumed to have a linear relationship,
    the model for $y_i$ is
    \begin{equation*}
        y_i = \beta_0 + \beta_1 x_i + \varepsilon_i
    \end{equation*}
    where $\varepsilon_i \sim \mathrm{N}\left( 0,\: \sigma^2 \right)$ is the residual.
    $s$ (RMS error) gives an estimate for $\sigma$ --
    how close the data is to the model.
    \subsubsection*{Estimates}
    \begin{align*}
        T_{\text{test}}:\frac{\hat{\beta}_0 - \beta_0}{s_{\hat{\beta}_0}} \sim t_{n-2},\quad \frac{\hat{\beta}_1 - \beta_1}{s_{\hat{\beta}_1}} \sim t_{n-2}
    \end{align*}
    \subsubsection*{Inferences about $\beta_0$ and $\beta_1$}
    \begin{align*}
        CI_c & = \hat{\beta}_0 \pm t_{n-2,\: 1-\alpha/2} s_{\hat{\beta}_0} \\
        CI_c & = \hat{\beta}_1 \pm t_{n-2,\: 1-\alpha/2} s_{\hat{\beta}_1}
    \end{align*}
    where $s_{\hat{\beta}_0}$ and $s_{\hat{\beta}_1}$ are the standard errors (SE) for $\hat{\beta}_0$ and $\hat{\beta}_1$.
    $H_{0,\:\beta_0}:\beta_0=0$ tests whether the model crosses the origin.
    $H_{0,\:\beta_1}:\beta_1=0$ tests whether the model is constant.
    \subsubsection*{R-squared}
    The percentage of the observed variance in $y$ that is explained by the model.
    \subsubsection*{Residual Plots}
    Test the following two assumptions:
    \begin{enumerate}
        \item The relationship between $X$ and $Y$ is best modelled linearly
              -- clear indication of a non-linear trend suggests assumption is not valid.
        \item The variance of residuals is the same for all observations, (not affected by $y_i$)
              -- uneven width of residual suggests assumption is not valid.
              This may lead to inaccurate inferences.
    \end{enumerate}
\end{multicols}
\end{document}