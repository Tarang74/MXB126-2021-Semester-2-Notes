\documentclass{article}
\usepackage{template}
\usepackage{changepage} % Modify page width
\usepackage{multicol}

\usepackage{titlesec}

\geometry{
	a4paper,
	margin = 10mm
} 

\usepackage{textcomp, upquote}

\lstset{language=Matlab, upquote=true}
\lstset{morekeywords={randi, false, imshow, drawpolygon, polyarea, drawline, ones, imread, VideoWriter, XData, YData, getFrame, writeVideo, deg2rad, soundsc, resample, audiowrite, sind}}
\lstset{
    basicstyle = \ttfamily, columns = fullflexible, keepspaces = true
}

\pagenumbering{gobble}

\setlength\columnsep{4pt}
\setlength{\parindent}{0pt}
\setlength{\parskip}{0pt}

\titleformat*\subsection{\raggedright\bfseries\large}
\titleformat*\subsubsection{\raggedright\bfseries}

\begin{document}
\titlespacing*\subsubsection{0pt}{1ex}{1ex}
\titlespacing*\subsection{0pt}{0ex}{0ex}

\setlength{\abovecaptionskip}{-5pt}
\setlength{\textfloatsep}{0pt}

\setlength{\abovedisplayskip}{1pt}
\setlength{\belowdisplayskip}{1pt}

\begin{multicols}{3}
    \subsubsection*{MATLAB}
    \lstinline!function [output1, ...]!

    \mbox{}\hfill\lstinline!= func_name(input1, ...)!
    \subsubsection*{Partial Fraction Decomposition}
    \begin{align*}
        ax+b                & \to \frac{A}{ax+b}                                                \\
        \left(ax+b\right)^k & \to \frac{A_1}{ax+b} + \cdots + \frac{A_k}{\left( ax+b \right)^k}
    \end{align*}
    \begin{align*}
        ax^2+bx+c                    & \to \frac{A}{ax^2+bx+c}                              \\
        \left(ax^2+bx+c\right)^k     & \to                                                  \\
        \frac{A_1x+B_1}{ax^2+bx+c} + & \cdots + \frac{A_kx+B_k}{\left( ax^2+bx+c \right)^k}
    \end{align*}
    \subsection*{Differential Equations}
    \subsubsection*{Electrical Circuits}
    \begin{align*}
        \sum v_{loop} = 0 &  & \sum i_{node} = 0
    \end{align*}
    \begin{equation*}
        \displaystyle i = \dv{q}{t}
    \end{equation*}
    \subsubsection*{Voltage drop across various elements:}
    \begin{align*}
        v_R & = iR          & R & : \text{resistance}  \\
        v_C & = \frac{q}{C} & C & : \text{capacitance} \\
        v_L & = L \dv{i}{t} & L & : \text{inductance}
    \end{align*}
    \subsubsection*{Mechanical Systems}
    \begin{equation*}
        \sum F = \dv{p}{t}
    \end{equation*}
    where $p = mv$.
    \begin{align*}
        F_T & = \left( c - v \right) d_f \\
        F_g & = mg                       \\
        F_S & = -kx
    \end{align*}
    \subsubsection*{Separable ODEs}
    For $\dv{y}{t} = p(t) q(y)$:
    \begin{equation*}
        \int \frac{1}{q(y)} \dv{y}{t} \dd{t} = \int p(t) \dd{t}.
    \end{equation*}
    \subsubsection*{Linear ODEs}
    For $\dv{y}{t} + p(t)y = q(t)$, use the \textit{integrating factor}:
    $I(t) = \e^{\int p(t) \dd{t}}$, so that
    \begin{equation*}
        y(t) = \frac{1}{I(t)} \int I(t) q(t) \dd{t}.
    \end{equation*}
    \subsubsection*{Linearisation}
    \begin{align*}
        f(t)              & \approx f(t_0) + f'(t_0)(t-t_0)                     \\
        f\bigl(y(t)\bigr) & \approx f'\bigl(y(t_0)\bigr)\bigl(y(t)-y(t_0)\bigr) \\
                          & \phantom{\approx} +f\bigl(y(t_0)\bigr)
    \end{align*}
    \subsubsection*{Euler's Method}
    \begin{align*}
        y(t+h)           & = y(t) + hy'(t)                     \\
        \symbfit{y}(t+h) & = \symbfit{y}(t) + h\symbfit{y}'(t)
    \end{align*}
    \subsubsection*{Modified Euler's Method}
    \begin{align*}
        y(t+h)           & = y(t) + \frac{h}{2}\bigl( y'(t) + y'(t+h) \bigr)                               \\
        \symbfit{y}(t+h) & = \symbfit{y}(t) + \frac{h}{2}\bigl( \symbfit{y}'(t) + \symbfit{y}'(t+h) \bigr)
    \end{align*}
    where $y'(t+h)$ is determined using Euler's method.
    \subsubsection*{Second-Order ODEs}
    \begin{equation*}
        ay'' + by' + cy = F(t)
    \end{equation*}
    \textbf{Homogeneous:} $F(t) = 0$

    \textbf{Nonhomogeneous:} $F(t) \neq 0$
    \subsubsection*{Homogeneous ODEs}
    \begin{equation*}
        y_H(t) = \e^{\lambda t}
    \end{equation*}
    \subsubsection*{Real Distinct Roots}
    \begin{equation*}
        y_H(t) = c_1\e^{\lambda_1 t} + c_2\e^{\lambda_2 t}
    \end{equation*}
    \subsubsection*{Real Repeated Roots}
    \begin{equation*}
        y_H(t) = c_1\e^{\lambda t} + c_2 t\e^{\lambda t}
    \end{equation*}
    \subsubsection*{Complex Conjugate Roots}
    Given $\lambda = \alpha \pm \beta i$:
    \begin{equation*}
        y_H(t) = \e^{\alpha x}\bigl( c_1\cos{\left( \beta t \right)} + c_2 \sin{\left( \beta t \right)} \bigr)
    \end{equation*}
    \subsubsection*{Nonhomogeneous ODEs}
    \begin{equation*}
        y(t) = y_H(t) + y_P(t).
    \end{equation*}
    \subsubsection*{Method of Undetermined Coefficients}
    \emph{See table below.}
    Substitute $y_P$ into the nonhomogeneous ODE, and solve the undetermined coefficients.
    \subsubsection*{System of ODEs}
    \begin{equation*}
        \symbfit{y}' = \symbf{A} \symbfit{y}
    \end{equation*}
    \subsubsection*{Homogeneous System}
    \begin{equation*}
        \symbfit{y}_H = \symbfit{q}\e^{\lambda t}
    \end{equation*}
    $\lambda_i$ are the eigenvalues of $\symbfit{A}$, and $\symbfit{q}_i$ are the associated eigenvectors.
    \subsubsection*{Higher-Order ODEs}
    \begin{equation*}
        y^{\left( n \right)} + a_1 y^{\left( n-1 \right)} + \cdots + a_{n-1} y' + a_n y = 0
    \end{equation*}
    Let $y_1 = y$, $y_2 = y'$, \dots, $y_n = y^{\left( n-1 \right)}$
    so that $\symbfit{y}=\avec{y_1,\: y_2,\: \dots,\: y_n}$. Then
    \begin{equation*}
        \symbfit{y}' = \symbf{A} \symbfit{y}
    \end{equation*}
    where
    \begin{equation*}
        \symbf{A} =
        \mqty[
        0      & 1        & 0        & \cdots & 0      \\
        0      & 0        & 1        & \cdots & 0      \\
        \vdots & \vdots   & \vdots   & \ddots & \vdots \\
        0      & 0        & 0        & \cdots & 1      \\
        -a_n   & -a_{n-1} & -a_{n-2} & \cdots & -a_1
        ]
    \end{equation*}
    \subsection*{Probability}
    \subsubsection*{Events}
    \begin{equation*}
        \Pr{\left( A \right)} = \sum_{x\in A} p(x)
    \end{equation*}
    \begin{align*}
        \Pr{\left( A^C \right)} = \Pr{\left( \overline{A} \right)} = 1 - \Pr{\left( A \right)}
    \end{align*}
    \subsubsection*{Disjoint Events (Mutually Exclusive)}
    Events don't have outcomes in common.
    \begin{align*}
        A \cap B                   & = \varnothing \\
        \Pr{\left(A \cap B\right)} & = 0
    \end{align*}
    \subsubsection*{Unions}
    \begin{align*}
        \Pr{\left( A \cup B \right)} & = \Pr{\left( A \right)} + \Pr{\left( B \right)} \\
                                     & \phantom{=} - \Pr{\left( A \cap B \right)}      \\
                                     & = 1 - \Pr{\left( A^C \cap B^C \right)}
    \end{align*}
    \subsubsection*{Independent Events}
    Outcome of events do not influence each other. Joint probability:
    \begin{equation*}
        \Pr{\left(A \cap B\right)} = \Pr{\left( A \right)}\Pr{\left( B \right)}
    \end{equation*}
    \subsubsection*{Dependent (Conditional) Events}
    Outcome of event depends on the outcome of the other. Joint probability of $A$ given $B$:
    \begin{equation*}
        \Pr{\left(A \;\middle|\; B\right)} = \frac{\Pr{\left( A \cap B \right)}}{\Pr{\left( B \right)}}
    \end{equation*}
    \subsubsection*{Total Probability}
    For disjoint events $B_i$:
    \begin{align*}
        A                     & = \bigcup\limits_{i=1}^n \left( A \cap B_i \right)                            \\
        \Pr{\left( A \right)} & = \sum_{i=1}^n \Pr{\left( A \cap B_i \right)}                                 \\
        \Pr{\left( A \right)} & = \sum_{i=1}^n \Pr{\left( A \;\middle|\; B_i \right)} \Pr{\left( B_i \right)}
    \end{align*}
    \subsubsection*{Bayes' Theorem}
    \begin{equation*}
        \Pr{\left(B \;\middle|\; A\right)} = \frac{\Pr{\left( A \;\middle|\; B \right)}\Pr{\left( B \right)}}{\Pr{\left( A \right)}}
    \end{equation*}
    \subsection*{Summary Statistics}
    \subsubsection*{Discrete Random Variables}
    Has countably many outcomes. Distributed according to a
    Probability Mass Function (PMF):
    \begin{equation*}
        p(x) = \Pr{\left( X = x \right)}
    \end{equation*}
    \subsubsection*{Continuous Random Variables}
    Has an infinite number of individual outcomes. Distributed according to a
    Probability Density Function $f(x)$ where
    \begin{equation*}
        \Pr{\left( x_1 \leq X \leq x_2 \right)} = \int_{x_1}^{x_2} f(u) \dd{u}.
    \end{equation*}
    The Cumulative Density Function (CDF)
    is defined as:
    \begin{equation*}
        F(x) = \Pr{\left( X \leq x \right)} = \int_{-\infty}^x f(u) \dd{u}
    \end{equation*}
    \subsubsection*{Expectation}
    \begin{equation*}
        \mu = \E{\left( X \right)} =
        \begin{cases}
            \sum_\Omega x p(x) \\
            \int_\Omega x f(x) \dd{x}
        \end{cases}
    \end{equation*}
    \subsubsection*{Variation}
    \begin{align*}
        \sigma^2 = \Var{\left( X \right)} & =
        \begin{cases}
            \sum_\Omega \left( x - \mu \right)^2 p(x) \\
            \int_\Omega \left( x - \mu \right)^2 f(x) \dd{x}
        \end{cases}                                       \\
                                          & = \E{\left( X^2 \right)} - \E{\left( X \right)}^2
    \end{align*}
    \subsubsection*{Standard Deviation}
    \begin{equation*}
        \sigma = \sqrt{\Var{\left( X \right)}}
    \end{equation*}
    \subsubsection*{General Linear Combinations}
    Expectation:
    \begin{align*}
        \E{\left( aX+b \right)}  & = a\E{\left( X \right)} + b                     \\
        \E{\left( aX+bY \right)} & = a\E{\left( X \right)} + b\E{\left( Y \right)}
    \end{align*}
    Variation:
    \begin{align*}
        \Var{\left( aX+b \right)}  & = a^2\Var{\left( X \right)}                  \\
        \Var{\left( aX+bY \right)} & = a^2\Var{\left( X \right)}                  \\
                                   & \phantom{=} + b^2\Var{\left( Y \right)}      \\
                                   & \phantom{=} - 2ab\Cov{\left( X,\: Y \right)}
    \end{align*}
    where
    \begin{align*}
        \Cov{\left( X,\: Y \right)} & = \E{\left( XY \right)} - \E{\left( X \right)}\E{\left( Y \right)} \\
                                    & = \rho_{XY} \sqrt{\Var{\left( X \right)}\Var{\left( Y \right)}}.
    \end{align*}
    The correlation $\Corr{\left( X,\: Y \right)}$ or $\rho_{XY}$ is a constant that describes the statistical relationship
    between $X$ and $Y$. $-1 \leq \rho_{XY} \leq 1$.
\end{multicols}
\begin{table}[H]
    \centering
    \begin{tabular}{c | c}
        \toprule
        $F(t)$                                                             & $y_P(t)$                                                                \\
        \midrule
        a constant                                                         & A                                                                       \\
        a polynomial of degree $n$                                         & $\displaystyle \sum_{i = 0}^n A_i t^i$                                  \\
        $\e^{kt}$                                                          & $A \e^{kt}$                                                             \\
        $\cos{\left( \omega t \right)}$ or $\sin{\left( \omega t \right)}$ & $A_0 \cos{\left( \omega t \right)} + A_1 \sin{\left( \omega t \right)}$ \\
        a combination of the above                                         & a combination of the above                                              \\
        linearly dependent to $y_H(t)$                                     & multiply $y_P(t)$ by $t$ until linearly independent                     \\
        \bottomrule
    \end{tabular}
\end{table}
\end{document}